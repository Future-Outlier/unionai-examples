name: Test Examples

on:
  # Uncomment to enable automatic testing on push to main
  # push:
  #   branches: [ main ]
  # Uncomment to enable automatic testing on pull requests
  # pull_request:
  #  branches: [ main ]
  # Uncomment to enable daily scheduled testing
  # schedule:
  #   # Run daily at 2 AM UTC
  #   - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: true
        default: 'test-preview'
        type: choice
        options:
          - test-preview
          - test-local
          - test
      filter:
        description: 'Filter pattern for specific tests (optional, mutually exclusive with file)'
        required: false
        type: string
      file:
        description: 'Specific file path to test (optional, mutually exclusive with filter)'
        required: false
        type: string
      python_version:
        description: 'Python version to use'
        required: false
        default: '3.13'
        type: choice
        options:
          - '3.12'
          - '3.13'

jobs:
  test-examples:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.13']

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}

    - name: Create virtual environment and update Flyte
      run: |
        uv venv --python ${{ matrix.python-version }} .venv
        source .venv/bin/activate
        echo "Virtual environment created with Python ${{ matrix.python-version }}"

    - name: Update to latest Flyte version
      run: |
        source .venv/bin/activate
        make update-flyte

    - name: Clean previous test artifacts
      run: |
        source .venv/bin/activate
        make clean

    - name: Run tests
      env:
        GITHUB_ACTIONS: true
        # Flyte client secret for authentication (referenced in config template)
        FLYTE_CLIENT_SECRET: ${{ secrets.FLYTE_CLIENT_SECRET }}
      run: |
        source .venv/bin/activate
        # Build the make command with optional parameters
        MAKE_CMD="make ${{ github.event.inputs.test_mode || 'test-preview' }}"

        # Add FILE parameter if specified (takes precedence over FILTER)
        if [ -n "${{ github.event.inputs.file }}" ]; then
          MAKE_CMD="$MAKE_CMD FILE=\"${{ github.event.inputs.file }}\""
        elif [ -n "${{ github.event.inputs.filter }}" ]; then
          MAKE_CMD="$MAKE_CMD FILTER=\"${{ github.event.inputs.filter }}\""
        fi

        echo "🚀 Executing: $MAKE_CMD"
        eval $MAKE_CMD

    - name: Debug test output files
      if: always()
      run: |
        echo "📁 Files in test/ directory:"
        find test/ -type f || echo "No test directory found"
        echo "📁 Files in test/logs/ directory:"
        find test/logs/ -type f || echo "No test/logs directory found"
        echo "📁 Current directory contents:"
        ls -la

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always() && hashFiles('test/logs/*') != ''
      with:
        name: test-results-py${{ matrix.python-version }}
        path: |
          test/logs/
        retention-days: 30

    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always() && (hashFiles('test/reports/test_report.html') != '' || hashFiles('test/reports/test_report.json') != '')
      with:
        name: test-reports-py${{ matrix.python-version }}
        path: |
          test/reports/test_report.html
          test/reports/test_report.json
        retention-days: 30

  # Job to combine and publish results from all matrix runs
  publish-results:
    needs: test-examples
    runs-on: ubuntu-latest
    # Only run for manual workflow dispatches, not for PR dry-runs
    if: always() && github.event_name == 'workflow_dispatch' && needs.test-examples.result != 'cancelled'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Debug artifacts
      run: |
        echo "📁 Downloaded artifacts structure:"
        find artifacts/ -type f || echo "No artifacts directory found"
        ls -la artifacts/ || echo "Cannot list artifacts directory"

    - name: Combine test results
      run: |
        mkdir -p combined-results/logs
        mkdir -p combined-results/reports
        echo "📊 Combining test results from all Python versions..."
        
        # Copy all log files from test-results artifacts
        find artifacts/ -path "*/test-results-*" -name "*.log" -exec cp {} combined-results/logs/ \; || echo "No log files found"
        
        # Copy all report files from test-reports artifacts  
        find artifacts/ -path "*/test-reports-*" -name "test_report.json" -exec cp {} combined-results/reports/ \; || echo "No test_report.json files found"
        find artifacts/ -path "*/test-reports-*" -name "test_report.html" -exec cp {} combined-results/reports/ \; || echo "No test_report.html files found"
        
        echo "📁 Combined results structure:"
        echo "Logs directory:"
        ls -la combined-results/logs/ || echo "No logs found"
        echo "Reports directory:"
        ls -la combined-results/reports/ || echo "No reports found"

    - name: Create test summary
      run: |
        echo "# 🧪 Test Summary" > combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md
        echo "## Test Results by Python Version" >> combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md
        
        # Analyze JSON reports from the reports directory
        for report in combined-results/reports/test_report.json; do
          if [ -f "$report" ]; then
            echo "- **Report**: $(basename $report)" >> combined-results/SUMMARY.md
            if command -v jq >/dev/null 2>&1; then
              passed_count=$(jq -r '[.[] | select(.status=="passed")] | length' "$report" 2>/dev/null || echo "0")
              failed_count=$(jq -r '[.[] | select(.status=="failed")] | length' "$report" 2>/dev/null || echo "0")
              timeout_count=$(jq -r '[.[] | select(.status=="timeout")] | length' "$report" 2>/dev/null || echo "0")
              total_count=$(jq -r 'length' "$report" 2>/dev/null || echo "0")
              echo "  - **Status**: $passed_count passed, $failed_count failed, $timeout_count timeout" >> combined-results/SUMMARY.md
              echo "  - **Total**: $total_count tests" >> combined-results/SUMMARY.md
            else
              echo "  - **Status**: JSON analysis requires jq (not available)" >> combined-results/SUMMARY.md
            fi
            echo "" >> combined-results/SUMMARY.md
          fi
        done
        
        # Count log files
        log_count=$(find combined-results/logs/ -name "*.log" | wc -l)
        report_count=$(find combined-results/reports/ -name "*.json" -o -name "*.html" | wc -l)
        
        echo "## 📋 Artifacts Generated" >> combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md
        echo "- **Log files**: $log_count individual test logs" >> combined-results/SUMMARY.md
        echo "- **Report files**: $report_count JSON/HTML reports" >> combined-results/SUMMARY.md
        echo "- **Structure**:" >> combined-results/SUMMARY.md
        echo "  - \`logs/\` - Individual test execution logs" >> combined-results/SUMMARY.md
        echo "  - \`reports/\` - JSON and HTML formatted test reports" >> combined-results/SUMMARY.md

    - name: Upload combined results
      uses: actions/upload-artifact@v4
      with:
        name: combined-test-results
        path: combined-results/
        retention-days: 90