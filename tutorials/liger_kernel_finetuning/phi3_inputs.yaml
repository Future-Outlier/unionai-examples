experiment_args:
  - use_liger: true
  - use_liger: false
training_args:
  model_name: "microsoft/Phi-3-mini-4k-instruct"
  dataset: "tatsu-lab/alpaca"
  use_liger: true
  max_seq_length: 512
  bf16: true
  max_steps: 1000
  # num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  eval_strategy: "no"
  save_strategy: "no"
  learning_rate: 0.000006
  weight_decay: 0.05
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: False
  warmup_ratio: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 1
  include_num_input_tokens_seen: true
  report_to: "wandb"
  seed: 42
